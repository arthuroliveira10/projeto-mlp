# MLP para XOR - Perceptron Multicamadas

Este projeto implementa uma **Multi-Layer Perceptron (MLP)** para resolver o problema do XOR usando backpropagation.

## ğŸ“‹ DescriÃ§Ã£o

O problema XOR (OU Exclusivo) Ã© um problema clÃ¡ssico que demonstra a importÃ¢ncia de camadas ocultas em redes neurais. Uma rede neural de camada Ãºnica nÃ£o consegue resolver o XOR, mas com uma camada oculta Ã© possÃ­vel.

## ğŸ”§ Funcionamento do Algoritmo

### Arquitetura da Rede
- **Camada de entrada**: 2 neurÃ´nios (x1 e x2)
- **Camada oculta**: 2 neurÃ´nios (h1 e h2) com funÃ§Ã£o de ativaÃ§Ã£o sigmoid
- **Camada de saÃ­da**: 1 neurÃ´nio (y) com funÃ§Ã£o de ativaÃ§Ã£o sigmoid

### Estrutura dos Pesos
```
Entradas (x1, x2) â†’ Camada Oculta (h1, h2) â†’ SaÃ­da (y)

Pesos da camada oculta:
- w11: x1 â†’ h1
- w12: x1 â†’ h2  
- w21: x2 â†’ h1
- w22: x2 â†’ h2

Pesos da camada de saÃ­da:
- wh1: h1 â†’ y
- wh2: h2 â†’ y

Biases:
- b1: bias para h1
- b2: bias para h2
- b3: bias para y
```

### Processo de Treinamento

1. **Forward Propagation**: Calcula as saÃ­das da rede
   ```
   h1 = sigmoid(x1 * w11 + x2 * w21 + b1)
   h2 = sigmoid(x1 * w12 + x2 * w22 + b2)
   y = sigmoid(h1 * wh1 + h2 * wh2 + b3)
   ```

2. **Backward Propagation**: Calcula os gradientes e atualiza os pesos
   - Calcula o erro: `erro = saÃ­da_desejada - saÃ­da_calculada`
   - Propaga o erro de trÃ¡s para frente usando a regra da cadeia
   - Atualiza os pesos usando a fÃ³rmula: `peso_novo = peso_antigo + taxa_aprendizado * gradiente`

3. **RepetiÃ§Ã£o**: O processo Ã© repetido por 50.000 Ã©pocas

### FunÃ§Ã£o Sigmoid
```python
sigmoid(x) = 1 / (1 + e^(-x))
```

A funÃ§Ã£o sigmoid Ã© usada para:
- Garantir que as saÃ­das fiquem entre 0 e 1
- Permitir a derivaÃ§Ã£o necessÃ¡ria para o backpropagation

## ğŸ“Š Tabela XOR

| Entrada | SaÃ­da Esperada |
|---------|----------------|
| [0, 0]  | 0              |
| [0, 1]  | 1              |
| [1, 0]  | 1              |
| [1, 1]  | 0              |

## ğŸš€ Como Executar

### PrÃ©-requisitos
- Python 3.x
- NumPy

### InstalaÃ§Ã£o
```bash
pip install numpy
```

### ExecuÃ§Ã£o
```bash
python mlp_xor.py
```

### Resultado Esperado
```
Resultados:
[0, 0] -> 0
[0, 1] -> 1
[1, 0] -> 1
[1, 1] -> 0
```

## âš™ï¸ ParÃ¢metros

- **Taxa de Aprendizado (alpha)**: 0.05
- **NÃºmero de Ã‰pocas**: 50.000
- **InicializaÃ§Ã£o dos Pesos**: Valores aleatÃ³rios entre -1 e 1
- **FunÃ§Ã£o de AtivaÃ§Ã£o**: Sigmoid em todas as camadas

## ğŸ“š Conceitos Utilizados

1. **Forward Propagation**: PropagaÃ§Ã£o dos sinais da entrada atÃ© a saÃ­da
2. **Backward Propagation**: PropagaÃ§Ã£o do erro da saÃ­da atÃ© a entrada
3. **Gradient Descent**: AtualizaÃ§Ã£o dos pesos minimizando o erro
4. **Derivada da Sigmoid**: `s'(x) = s(x) * (1 - s(x))`

## ğŸ¯ Por que XOR Ã© Importante?

O XOR Ã© um problema nÃ£o-linear que nÃ£o pode ser resolvido por um perceptron de camada Ãºnica. Este problema foi fundamental no desenvolvimento das redes neurais multicamadas e do algoritmo backpropagation, demonstrando a necessidade de camadas ocultas para resolver problemas complexos.

## ğŸ“ Estrutura do CÃ³digo

```python
class MLP:
    - train(): Treina a rede usando backpropagation
    - predict(): Faz prediÃ§Ãµes apÃ³s o treinamento
```

## ğŸ“ˆ Resultados

A rede aprende corretamente a funÃ§Ã£o XOR apÃ³s 50.000 Ã©pocas de treinamento, demonstrando a capacidade de uma MLP com camadas ocultas de resolver problemas nÃ£o-linearmente separÃ¡veis.

